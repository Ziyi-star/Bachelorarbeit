{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f163eb8",
   "metadata": {},
   "source": [
    "## Combine all datasets and Train Test Spilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c29d18e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0e0e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "sys.path.append('../../')   # Add parent directory to Python path\n",
    "from utils.preprocessing import *\n",
    "from utils.segmentation import *\n",
    "from utils.visualization import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)  # For reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfe69668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_samples(data, n=1):\n",
    "    indices = np.random.choice(data.shape[0], n, replace=False)\n",
    "    return data[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71deede5",
   "metadata": {},
   "source": [
    "## 1. Combine all datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da902cc",
   "metadata": {},
   "source": [
    "### 1.1 Curb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de676f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P3\n",
    "data = np.load('../../data/Curb/P3/handlebar/Accelerometer/segments_100hz_0.5s_50overlap.npz')\n",
    "curb_p3_0 = data['segments_0']\n",
    "curb_p3_1 = data['segments_1']\n",
    "# P6\n",
    "data = np.load('../../data/Curb/P6/handlebar/Accelerometer/Accelerometer_data_combined_100hz_scene0_segments.npz')\n",
    "curb_p6_0 = data['segments']\n",
    "data = np.load('../../data/Curb/P6/handlebar/Accelerometer/Accelerometer_data_combined_100hz_scene1_segments.npz')\n",
    "curb_p6_1 = data['segments']\n",
    "# P11\n",
    "data = np.load('../../data/Curb/P11/handlebar/Accelerometer/Accelerometer_data_combined_100hz_scene0_segments.npz')\n",
    "curb_p11_0 = data['segments']\n",
    "data = np.load('../../data/Curb/P11/handlebar/Accelerometer/Accelerometer_data_combined_100hz_scene1_segments.npz')\n",
    "curb_p11_1 = data['segments']\n",
    "# P12\n",
    "data = np.load('../../data/Curb/P12/handlebar/Accelerometer/Accelerometer_data_combined_100hz_scene0_segments.npz')\n",
    "curb_p12_0 = data['segments']\n",
    "data = np.load('../../data/Curb/P12/handlebar/Accelerometer/Accelerometer_data_combined_100hz_scene1_segments.npz')\n",
    "curb_p12_1 = data['segments']\n",
    "# P18\n",
    "data = np.load('../../data/Curb/P18/handlebar/Accelerometer/Accelerometer_data_combined_100hz_scene0_segments.npz')\n",
    "curb_p18_0 = data['segments']\n",
    "data = np.load('../../data/Curb/P18/handlebar/Accelerometer/Accelerometer_data_combined_100hz_scene1_segments.npz')\n",
    "curb_p18_1 = data['segments']\n",
    "# P21\n",
    "data = np.load('../../data/Curb/P21/handlebar/Accelerometer/Accelerometer_data_combined_100hz_scene0_segments.npz')\n",
    "curb_p21_0 = data['segments']\n",
    "data = np.load('../../data/Curb/P21/handlebar/Accelerometer/Accelerometer_data_combined_100hz_scene1_segments.npz')\n",
    "curb_p21_1 = data['segments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d74e335f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After balancing:\n",
      "Curb P3 (scene 0): (188, 50, 3)\n",
      "Curb P3 (scene 1): (188, 50, 3)\n",
      "Curb P6 (scene 0): (167, 50, 3)\n",
      "Curb P6 (scene 1): (167, 50, 3)\n",
      "Curb P11 (scene 0): (250, 50, 3)\n",
      "Curb P11 (scene 1): (250, 50, 3)\n",
      "Curb P12 (scene 0): (193, 50, 3)\n",
      "Curb P12 (scene 1): (193, 50, 3)\n",
      "Curb P18 (scene 0): (167, 50, 3)\n",
      "Curb P18 (scene 1): (167, 50, 3)\n",
      "Curb P21 (scene 0): (224, 50, 3)\n",
      "Curb P21 (scene 1): (224, 50, 3)\n",
      "Total curb scene 0 samples: 1189\n",
      "Total curb scene 1 samples: 1189\n"
     ]
    }
   ],
   "source": [
    "# After loading all the curb datasets\n",
    "# 1. First get the count for each participant's scene 1 data\n",
    "p6_count = curb_p6_1.shape[0]\n",
    "p11_count = curb_p11_1.shape[0]\n",
    "p12_count = curb_p12_1.shape[0]\n",
    "p18_count = curb_p18_1.shape[0]\n",
    "p21_count = curb_p21_1.shape[0]\n",
    "\n",
    "# 2. Randomly select the same number of samples from scene 0 data\n",
    "curb_p6_0 = select_random_samples(curb_p6_0, p6_count)\n",
    "curb_p11_0 = select_random_samples(curb_p11_0, p11_count)\n",
    "curb_p12_0 = select_random_samples(curb_p12_0, p12_count)\n",
    "curb_p18_0 = select_random_samples(curb_p18_0, p18_count)\n",
    "curb_p21_0 = select_random_samples(curb_p21_0, p21_count)\n",
    "\n",
    "# 3. Print shape to verify\n",
    "print(\"After balancing:\")\n",
    "print(\"Curb P3 (scene 0):\", curb_p3_0.shape)\n",
    "print(\"Curb P3 (scene 1):\", curb_p3_1.shape)\n",
    "print(\"Curb P6 (scene 0):\", curb_p6_0.shape)\n",
    "print(\"Curb P6 (scene 1):\", curb_p6_1.shape)\n",
    "print(\"Curb P11 (scene 0):\", curb_p11_0.shape)\n",
    "print(\"Curb P11 (scene 1):\", curb_p11_1.shape)\n",
    "print(\"Curb P12 (scene 0):\", curb_p12_0.shape)\n",
    "print(\"Curb P12 (scene 1):\", curb_p12_1.shape)\n",
    "print(\"Curb P18 (scene 0):\", curb_p18_0.shape)\n",
    "print(\"Curb P18 (scene 1):\", curb_p18_1.shape)\n",
    "print(\"Curb P21 (scene 0):\", curb_p21_0.shape)\n",
    "print(\"Curb P21 (scene 1):\", curb_p21_1.shape)\n",
    "\n",
    "# 4. Combine all curb data\n",
    "data_curb_0 = np.concatenate([curb_p3_0, curb_p6_0, curb_p11_0, curb_p12_0, curb_p18_0, curb_p21_0])\n",
    "data_curb_1 = np.concatenate([curb_p3_1, curb_p6_1, curb_p11_1, curb_p12_1, curb_p18_1, curb_p21_1])\n",
    "\n",
    "print(f\"Total curb scene 0 samples: {data_curb_0.shape[0]}\")\n",
    "print(f\"Total curb scene 1 samples: {data_curb_1.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f6cc2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the combined curb data\n",
    "with open('../data_more/curb_0.5s_combined_all.pkl', 'wb') as f:\n",
    "    pickle.dump({'scene_0': data_curb_0, 'scene_1': data_curb_1}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2c8137",
   "metadata": {},
   "source": [
    "### 1.2 Asphalt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcc4a130",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P1\n",
    "data= np.load('../../data/RoadRoughness/Raw/Asphalt/P1/segments_100hz_0.5s_50overlap.npz')\n",
    "asphalt_p1= data['segments']\n",
    "#P2\n",
    "data= np.load('../../data/RoadRoughness/Raw/Asphalt/P2/segments_100hz_0.5s_50overlap.npz')\n",
    "asphalt_p2= data['segments']\n",
    "# P3\n",
    "data = np.load('../../data/RoadRoughness/Raw/Asphalt/P3/segments_100hz_0.5s_50overlap.npz')\n",
    "asphalt_p3 = data['segments']\n",
    "# P4\n",
    "data = np.load('../../data/RoadRoughness/Raw/Asphalt/P4/segments_100hz_0.5s_50overlap.npz')\n",
    "asphalt_p4 = data['segments']\n",
    "# P5\n",
    "data = np.load('../../data/RoadRoughness/Raw/Asphalt/P5/segments_100hz_0.5s_50overlap.npz')\n",
    "asphalt_p5 = data['segments']\n",
    "# P6\n",
    "data = np.load('../../data/RoadRoughness/Raw/Asphalt/P6/segments_100hz_0.5s_50overlap.npz')\n",
    "asphalt_p6 = data['segments']\n",
    "# P7\n",
    "data = np.load('../../data/RoadRoughness/Raw/Asphalt/P7/segments_100hz_0.5s_50overlap.npz')\n",
    "asphalt_p7 = data['segments']\n",
    "# P8\n",
    "data = np.load('../../data/RoadRoughness/Raw/Asphalt/P8/segments_100hz_0.5s_50overlap.npz')\n",
    "asphalt_p8 = data['segments']\n",
    "# P9\n",
    "data = np.load('../../data/RoadRoughness/Raw/Asphalt/P9/segments_100hz_0.5s_50overlap.npz')\n",
    "asphalt_p9 = data['segments']\n",
    "# P10\n",
    "data = np.load('../../data/RoadRoughness/Raw/Asphalt/P10/segments_100hz_0.5s_50overlap.npz')\n",
    "asphalt_p10 = data['segments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbbf43b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1: (74, 50, 3)\n",
      "P2: (120, 50, 3)\n",
      "P3: (81, 50, 3)\n",
      "P4: (74, 50, 3)\n",
      "P5: (114, 50, 3)\n",
      "P6: (106, 50, 3)\n",
      "P7: (78, 50, 3)\n",
      "P8: (86, 50, 3)\n",
      "P9: (190, 50, 3)\n",
      "P10: (93, 50, 3)\n",
      "\n",
      "Total samples: 1016\n"
     ]
    }
   ],
   "source": [
    "# Print shape of all asphalt datasets\n",
    "print(\"P1:\", asphalt_p1.shape)\n",
    "print(\"P2:\", asphalt_p2.shape)\n",
    "print(\"P3:\", asphalt_p3.shape)\n",
    "print(\"P4:\", asphalt_p4.shape)\n",
    "print(\"P5:\", asphalt_p5.shape)\n",
    "print(\"P6:\", asphalt_p6.shape)\n",
    "print(\"P7:\", asphalt_p7.shape)\n",
    "print(\"P8:\", asphalt_p8.shape)\n",
    "print(\"P9:\", asphalt_p9.shape)\n",
    "print(\"P10:\", asphalt_p10.shape)\n",
    "print(\"\\nTotal samples:\", sum(arr.shape[0] for arr in [asphalt_p1, asphalt_p2, asphalt_p3, asphalt_p4, \n",
    "                                                       asphalt_p5, asphalt_p6, asphalt_p7, asphalt_p8,\n",
    "                                                       asphalt_p9, asphalt_p10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4378b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine and save all asphalt data\n",
    "data_asphalt = np.concatenate([asphalt_p1, asphalt_p2, asphalt_p3, asphalt_p4, asphalt_p5, \n",
    "                              asphalt_p6, asphalt_p7, asphalt_p8, asphalt_p9, asphalt_p10])\n",
    "with open('../data_more/asphalt_0.5s_combined_all.pkl', 'wb') as f:\n",
    "    pickle.dump(data_asphalt, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91db8c6",
   "metadata": {},
   "source": [
    "### 1.3 Cobblestone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1bc43685",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P1\n",
    "data = np.load('../../data/RoadRoughness/Raw/Cobblestone/P1/segments_100hz_0.5s_50overlap.npz')\n",
    "cobblestone_p1 = data['segments']\n",
    "#P2\n",
    "data = np.load('../../data/RoadRoughness/Raw/Cobblestone/P2/segments_100hz_0.5s_50overlap.npz')\n",
    "cobblestone_p2 = data['segments']\n",
    "# P3\n",
    "data = np.load('../../data/RoadRoughness/Raw/Cobblestone/P3/segments_100hz_0.5s_50overlap.npz')\n",
    "cobblestone_p3 = data['segments']\n",
    "# P4\n",
    "data = np.load('../../data/RoadRoughness/Raw/Cobblestone/P4/segments_100hz_0.5s_50overlap.npz')\n",
    "cobblestone_p4 = data['segments']\n",
    "# P5\n",
    "data = np.load('../../data/RoadRoughness/Raw/Cobblestone/P5/segments_100hz_0.5s_50overlap.npz')\n",
    "cobblestone_p5 = data['segments']\n",
    "# P6\n",
    "data = np.load('../../data/RoadRoughness/Raw/Cobblestone/P6/segments_100hz_0.5s_50overlap.npz')\n",
    "cobblestone_p6 = data['segments']\n",
    "# P7\n",
    "data = np.load('../../data/RoadRoughness/Raw/Cobblestone/P7/segments_100hz_0.5s_50overlap.npz')\n",
    "cobblestone_p7 = data['segments']\n",
    "# P8\n",
    "data = np.load('../../data/RoadRoughness/Raw/Cobblestone/P8/segments_100hz_0.5s_50overlap.npz')\n",
    "cobblestone_p8 = data['segments']\n",
    "# P9\n",
    "data = np.load('../../data/RoadRoughness/Raw/Cobblestone/P9/segments_100hz_0.5s_50overlap.npz')\n",
    "cobblestone_p9 = data['segments']\n",
    "# P10\n",
    "data = np.load('../../data/RoadRoughness/Raw/Cobblestone/P10/segments_100hz_0.5s_50overlap.npz')\n",
    "cobblestone_p10 = data['segments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9366b139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cobblestone shapes:\n",
      "P1: (59, 50, 3)\n",
      "P2: (55, 50, 3)\n",
      "P3: (49, 50, 3)\n",
      "P4: (71, 50, 3)\n",
      "P5: (51, 50, 3)\n",
      "P6: (62, 50, 3)\n",
      "P7: (47, 50, 3)\n",
      "P8: (67, 50, 3)\n",
      "P9: (67, 50, 3)\n",
      "P10: (75, 50, 3)\n",
      "\n",
      "Total samples: 603\n"
     ]
    }
   ],
   "source": [
    "# Print shape of all cobblestone datasets\n",
    "print(\"Cobblestone shapes:\")\n",
    "print(\"P1:\", cobblestone_p1.shape)\n",
    "print(\"P2:\", cobblestone_p2.shape)\n",
    "print(\"P3:\", cobblestone_p3.shape)\n",
    "print(\"P4:\", cobblestone_p4.shape)\n",
    "print(\"P5:\", cobblestone_p5.shape)\n",
    "print(\"P6:\", cobblestone_p6.shape)\n",
    "print(\"P7:\", cobblestone_p7.shape)\n",
    "print(\"P8:\", cobblestone_p8.shape)\n",
    "print(\"P9:\", cobblestone_p9.shape)\n",
    "print(\"P10:\", cobblestone_p10.shape)\n",
    "print(\"\\nTotal samples:\", sum(arr.shape[0] for arr in [cobblestone_p1, cobblestone_p2, cobblestone_p3, cobblestone_p4, \n",
    "                                                      cobblestone_p5, cobblestone_p6, cobblestone_p7, cobblestone_p8,\n",
    "                                                      cobblestone_p9, cobblestone_p10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "477221f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine and save all cobblestone data\n",
    "data_cobblestone = np.concatenate([cobblestone_p1, cobblestone_p2, cobblestone_p3, cobblestone_p4, cobblestone_p5, \n",
    "                                  cobblestone_p6, cobblestone_p7, cobblestone_p8, cobblestone_p9, cobblestone_p10])\n",
    "with open('../data_more/cobblestone_0.5s_combined_all.pkl', 'wb') as f:\n",
    "    pickle.dump(data_cobblestone, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eee9730",
   "metadata": {},
   "source": [
    "### 1.4 CompactGravel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f53a9f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P1\n",
    "data = np.load('../../data/RoadRoughness/Raw/CompactGravel/P1/segments_100hz_0.5s_50overlap.npz')\n",
    "compactgravel_p1 = data['segments']\n",
    "#P2\n",
    "data = np.load('../../data/RoadRoughness/Raw/CompactGravel/P2/segments_100hz_0.5s_50overlap.npz')\n",
    "compactgravel_p2 = data['segments']\n",
    "# P3\n",
    "data = np.load('../../data/RoadRoughness/Raw/CompactGravel/P3/segments_100hz_0.5s_50overlap.npz')\n",
    "compactgravel_p3 = data['segments']\n",
    "# P4\n",
    "data = np.load('../../data/RoadRoughness/Raw/CompactGravel/P4/segments_100hz_0.5s_50overlap.npz')\n",
    "compactgravel_p4 = data['segments']\n",
    "# P5\n",
    "data = np.load('../../data/RoadRoughness/Raw/CompactGravel/P5/segments_100hz_0.5s_50overlap.npz')\n",
    "compactgravel_p5 = data['segments']\n",
    "# P6\n",
    "data = np.load('../../data/RoadRoughness/Raw/CompactGravel/P6/segments_100hz_0.5s_50overlap.npz')\n",
    "compactgravel_p6 = data['segments']\n",
    "# P7\n",
    "data = np.load('../../data/RoadRoughness/Raw/CompactGravel/P7/segments_100hz_0.5s_50overlap.npz')\n",
    "compactgravel_p7 = data['segments']\n",
    "# P8\n",
    "data = np.load('../../data/RoadRoughness/Raw/CompactGravel/P8/segments_100hz_0.5s_50overlap.npz')\n",
    "compactgravel_p8 = data['segments']\n",
    "# P9\n",
    "data = np.load('../../data/RoadRoughness/Raw/CompactGravel/P9/segments_100hz_0.5s_50overlap.npz')\n",
    "compactgravel_p9 = data['segments']\n",
    "# P10\n",
    "data = np.load('../../data/RoadRoughness/Raw/CompactGravel/P10/segments_100hz_0.5s_50overlap.npz')\n",
    "compactgravel_p10 = data['segments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93233cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompactGravel shapes:\n",
      "P1: (66, 50, 3)\n",
      "P2: (71, 50, 3)\n",
      "P3: (99, 50, 3)\n",
      "P4: (59, 50, 3)\n",
      "P5: (51, 50, 3)\n",
      "P6: (59, 50, 3)\n",
      "P7: (79, 50, 3)\n",
      "P8: (64, 50, 3)\n",
      "P9: (61, 50, 3)\n",
      "P10: (79, 50, 3)\n",
      "\n",
      "Total samples: 688\n"
     ]
    }
   ],
   "source": [
    "# Print shape of all compact gravel datasets\n",
    "print(\"CompactGravel shapes:\")\n",
    "print(\"P1:\", compactgravel_p1.shape)\n",
    "print(\"P2:\", compactgravel_p2.shape)\n",
    "print(\"P3:\", compactgravel_p3.shape)\n",
    "print(\"P4:\", compactgravel_p4.shape)\n",
    "print(\"P5:\", compactgravel_p5.shape)\n",
    "print(\"P6:\", compactgravel_p6.shape)\n",
    "print(\"P7:\", compactgravel_p7.shape)\n",
    "print(\"P8:\", compactgravel_p8.shape)\n",
    "print(\"P9:\", compactgravel_p9.shape)\n",
    "print(\"P10:\", compactgravel_p10.shape)\n",
    "print(\"\\nTotal samples:\", sum(arr.shape[0] for arr in [compactgravel_p1, compactgravel_p2, compactgravel_p3, compactgravel_p4, \n",
    "                                                      compactgravel_p5, compactgravel_p6, compactgravel_p7, compactgravel_p8,\n",
    "                                                      compactgravel_p9, compactgravel_p10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f09e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine and save all compact gravel data\n",
    "# data_compactgravel = np.concatenate([compactgravel_p1, compactgravel_p2, compactgravel_p3, compactgravel_p4, compactgravel_p5, \n",
    "#                                    compactgravel_p6, compactgravel_p7, compactgravel_p8, compactgravel_p9, compactgravel_p10])\n",
    "# with open('../data_more/compactgravel_0.5s_combined_all.pkl', 'wb') as f:\n",
    "#     pickle.dump(data_compactgravel, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14142b3e",
   "metadata": {},
   "source": [
    "### 1.5 Dirt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05e5bfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P1\n",
    "data = np.load('../../data/RoadRoughness/Raw/Dirt/P1/segments_100hz_0.5s_50overlap.npz')\n",
    "dirt_p1 = data['segments']\n",
    "#P2\n",
    "data = np.load('../../data/RoadRoughness/Raw/Dirt/P2/segments_100hz_0.5s_50overlap.npz')\n",
    "dirt_p2 = data['segments']\n",
    "# P3\n",
    "data = np.load('../../data/RoadRoughness/Raw/Dirt/P3/segments_100hz_0.5s_50overlap.npz')\n",
    "dirt_p3 = data['segments']\n",
    "# P4\n",
    "data = np.load('../../data/RoadRoughness/Raw/Dirt/P4/segments_100hz_0.5s_50overlap.npz')\n",
    "dirt_p4 = data['segments']\n",
    "# P5\n",
    "data = np.load('../../data/RoadRoughness/Raw/Dirt/P5/segments_100hz_0.5s_50overlap.npz')\n",
    "dirt_p5 = data['segments']\n",
    "# P6\n",
    "data = np.load('../../data/RoadRoughness/Raw/Dirt/P6/segments_100hz_0.5s_50overlap.npz')\n",
    "dirt_p6 = data['segments']\n",
    "# P7\n",
    "data = np.load('../../data/RoadRoughness/Raw/Dirt/P7/segments_100hz_0.5s_50overlap.npz')\n",
    "dirt_p7 = data['segments']\n",
    "# P8\n",
    "data = np.load('../../data/RoadRoughness/Raw/Dirt/P8/segments_100hz_0.5s_50overlap.npz')\n",
    "dirt_p8 = data['segments']\n",
    "# P9\n",
    "data = np.load('../../data/RoadRoughness/Raw/Dirt/P9/segments_100hz_0.5s_50overlap.npz')\n",
    "dirt_p9 = data['segments']\n",
    "# P10\n",
    "data = np.load('../../data/RoadRoughness/Raw/Dirt/P10/segments_100hz_0.5s_50overlap.npz')\n",
    "dirt_p10 = data['segments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2812cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dirt shapes:\n",
      "P1: (93, 50, 3)\n",
      "P2: (74, 50, 3)\n",
      "P3: (75, 50, 3)\n",
      "P4: (89, 50, 3)\n",
      "P5: (63, 50, 3)\n",
      "P6: (100, 50, 3)\n",
      "P7: (67, 50, 3)\n",
      "P8: (79, 50, 3)\n",
      "P9: (83, 50, 3)\n",
      "P10: (67, 50, 3)\n",
      "\n",
      "Total samples: 790\n"
     ]
    }
   ],
   "source": [
    "# Print shape of all dirt datasets\n",
    "print(\"Dirt shapes:\")\n",
    "print(\"P1:\", dirt_p1.shape)\n",
    "print(\"P2:\", dirt_p2.shape)\n",
    "print(\"P3:\", dirt_p3.shape)\n",
    "print(\"P4:\", dirt_p4.shape)\n",
    "print(\"P5:\", dirt_p5.shape)\n",
    "print(\"P6:\", dirt_p6.shape)\n",
    "print(\"P7:\", dirt_p7.shape)\n",
    "print(\"P8:\", dirt_p8.shape)\n",
    "print(\"P9:\", dirt_p9.shape)\n",
    "print(\"P10:\", dirt_p10.shape)\n",
    "print(\"\\nTotal samples:\", sum(arr.shape[0] for arr in [dirt_p1, dirt_p2, dirt_p3, dirt_p4, \n",
    "                                                      dirt_p5, dirt_p6, dirt_p7, dirt_p8,\n",
    "                                                      dirt_p9, dirt_p10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e51030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine and save all dirt data\n",
    "# data_dirt = np.concatenate([dirt_p1, dirt_p2, dirt_p3, dirt_p4, dirt_p5, \n",
    "#                            dirt_p6, dirt_p7, dirt_p8, dirt_p9, dirt_p10])\n",
    "# with open('../data_more/dirt_0.5s_combined_all.pkl', 'wb') as f:\n",
    "#     pickle.dump(data_dirt, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd2330a",
   "metadata": {},
   "source": [
    "### 1.6 PavingStone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a79757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P1\n",
    "data = np.load('../../data/RoadRoughness/Raw/PavingStone/P1/segments_100hz_0.5s_50overlap.npz')\n",
    "pavingstone_p1 = data['segments']\n",
    "#P2\n",
    "data = np.load('../../data/RoadRoughness/Raw/PavingStone/P2/segments_100hz_0.5s_50overlap.npz')\n",
    "pavingstone_p2 = data['segments']\n",
    "# P3\n",
    "data = np.load('../../data/RoadRoughness/Raw/PavingStone/P3/segments_100hz_0.5s_50overlap.npz')\n",
    "pavingstone_p3 = data['segments']\n",
    "# P4\n",
    "data = np.load('../../data/RoadRoughness/Raw/PavingStone/P4/segments_100hz_0.5s_50overlap.npz')\n",
    "pavingstone_p4 = data['segments']\n",
    "# P5\n",
    "data = np.load('../../data/RoadRoughness/Raw/PavingStone/P5/segments_100hz_0.5s_50overlap.npz')\n",
    "pavingstone_p5 = data['segments']\n",
    "# P6\n",
    "data = np.load('../../data/RoadRoughness/Raw/PavingStone/P6/segments_100hz_0.5s_50overlap.npz')\n",
    "pavingstone_p6 = data['segments']\n",
    "# P7\n",
    "data = np.load('../../data/RoadRoughness/Raw/PavingStone/P7/segments_100hz_0.5s_50overlap.npz')\n",
    "pavingstone_p7 = data['segments']\n",
    "# P8\n",
    "data = np.load('../../data/RoadRoughness/Raw/PavingStone/P8/segments_100hz_0.5s_50overlap.npz')\n",
    "pavingstone_p8 = data['segments']\n",
    "# P9\n",
    "data = np.load('../../data/RoadRoughness/Raw/PavingStone/P9/segments_100hz_0.5s_50overlap.npz')\n",
    "pavingstone_p9 = data['segments']\n",
    "# P10\n",
    "data = np.load('../../data/RoadRoughness/Raw/PavingStone/P10/segments_100hz_0.5s_50overlap.npz')\n",
    "pavingstone_p10 = data['segments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "389bccdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PavingStone shapes:\n",
      "P1: (81, 50, 3)\n",
      "P2: (83, 50, 3)\n",
      "P3: (72, 50, 3)\n",
      "P4: (67, 50, 3)\n",
      "P5: (57, 50, 3)\n",
      "P6: (89, 50, 3)\n",
      "P7: (80, 50, 3)\n",
      "P8: (83, 50, 3)\n",
      "P9: (75, 50, 3)\n",
      "P10: (79, 50, 3)\n",
      "\n",
      "Total samples: 766\n"
     ]
    }
   ],
   "source": [
    "# Print shape of all paving stone datasets\n",
    "print(\"PavingStone shapes:\")\n",
    "print(\"P1:\", pavingstone_p1.shape)\n",
    "print(\"P2:\", pavingstone_p2.shape)\n",
    "print(\"P3:\", pavingstone_p3.shape)\n",
    "print(\"P4:\", pavingstone_p4.shape)\n",
    "print(\"P5:\", pavingstone_p5.shape)\n",
    "print(\"P6:\", pavingstone_p6.shape)\n",
    "print(\"P7:\", pavingstone_p7.shape)\n",
    "print(\"P8:\", pavingstone_p8.shape)\n",
    "print(\"P9:\", pavingstone_p9.shape)\n",
    "print(\"P10:\", pavingstone_p10.shape)\n",
    "print(\"\\nTotal samples:\", sum(arr.shape[0] for arr in [pavingstone_p1, pavingstone_p2, pavingstone_p3, pavingstone_p4, \n",
    "                                                      pavingstone_p5, pavingstone_p6, pavingstone_p7, pavingstone_p8,\n",
    "                                                      pavingstone_p9, pavingstone_p10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a43b693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine and save all paving stone data\n",
    "# data_pavingstone = np.concatenate([pavingstone_p1, pavingstone_p2, pavingstone_p3, pavingstone_p4, pavingstone_p5, \n",
    "#                                  pavingstone_p6, pavingstone_p7, pavingstone_p8, pavingstone_p9, pavingstone_p10])\n",
    "# with open('../data_more/pavingstone_0.5s_combined_all.pkl', 'wb') as f:\n",
    "#     pickle.dump(data_pavingstone, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
